<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MACE: Mass Concept Erasure in Diffusion Models | My New Hugo Site</title>
<meta name=keywords content="mace"><meta name=description content="Desc Text."><meta name=author content="Me"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as=style><link rel=icon href=https://example.org/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://example.org/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://example.org/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://example.org/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://example.org/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://example.org/posts/mace/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="MACE: Mass Concept Erasure in Diffusion Models"><meta property="og:description" content="Desc Text."><meta property="og:type" content="article"><meta property="og:url" content="https://example.org/posts/mace/"><meta property="og:image" content="https://example.org/%3Cimage%20path/url%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-09-15T11:30:03+00:00"><meta property="article:modified_time" content="2020-09-15T11:30:03+00:00"><meta property="og:site_name" content="ExampleSite"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://example.org/%3Cimage%20path/url%3E"><meta name=twitter:title content="MACE: Mass Concept Erasure in Diffusion Models"><meta name=twitter:description content="Desc Text."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://example.org/posts/"},{"@type":"ListItem","position":2,"name":"MACE: Mass Concept Erasure in Diffusion Models","item":"https://example.org/posts/mace/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MACE: Mass Concept Erasure in Diffusion Models","name":"MACE: Mass Concept Erasure in Diffusion Models","description":"Desc Text.","keywords":["mace"],"articleBody":" Figure 1: Overview of MACE framework\nTLDR MACE is a novel framework for erasing multiple concepts from text-to-image diffusion models It achieves superior balance between generality and specificity compared to existing methods MACE can effectively erase up to 100 concepts simultaneously without compromising model performance The approach combines closed-form cross-attention refinement and concept-specific LoRA modules MACE outperforms state-of-the-art methods across various concept erasure tasks Introduction Text-to-image diffusion models have revolutionized the field of AI-generated art and content creation. However, their ability to generate almost any concept based on text prompts has raised concerns about potential misuse, copyright infringement, and the creation of harmful or explicit content. To address these issues, researchers have been exploring ways to selectively remove or “erase” specific concepts from these models.\nIn this blog post, we’ll dive into a cutting-edge approach called MACE (MAss Concept Erasure) proposed by Shilin Lu et al. in their paper “MACE: Mass Concept Erasure in Diffusion Models” [1]. This work introduces a powerful framework for erasing multiple concepts from text-to-image diffusion models while maintaining a delicate balance between generality and specificity.\nThe Challenge of Concept Erasure Before we delve into the details of MACE, let’s understand the key challenges in concept erasure:\nEfficacy: The model should not generate images related to the erased concept when prompted. Generality: The erasure should extend to synonyms and related terms of the target concept. Specificity: Unrelated concepts should remain intact and generate as expected. Previous methods have struggled to achieve all three of these goals simultaneously, especially when dealing with multiple concepts. MACE addresses these challenges through a novel combination of techniques.\nThe MACE Framework MACE consists of three main components:\nClosed-Form Cross-Attention Refinement Target Concept Erasure with LoRA Fusion of Multi-LoRA Modules Let’s explore each of these components in detail.\n1. Closed-Form Cross-Attention Refinement The first step in MACE is to refine the cross-attention layers of the pretrained model. This is crucial because the information of a concept is not only contained within its own tokens but also embedded in co-existing words through the attention mechanism.\nFigure 2: A concept can be generated solely via residual information\nTo address this, MACE uses a closed-form solution to update the projection matrices $\\mathbf{W}_k$ and $\\mathbf{W}_v$ in the cross-attention layers. The objective is to discourage the model from embedding residual information of the target phrase into other words.\nThe optimization problem is formulated as:\n$$ \\min_{\\mathbf{W}^{\\prime}k} \\sum{i=1}^n | \\mathbf{W}_k^{\\prime} \\cdot \\mathbf{e}^f_i - \\mathbf{W}_k \\cdot \\mathbf{e}_i^g |2^2 + \\lambda_1 \\sum{i=n+1}^{n+m} | \\mathbf{W}_k^{\\prime} \\cdot \\mathbf{e}^p_i - \\mathbf{W}_k \\cdot \\mathbf{e}_i^p |_2^2 $$\nWhere:\n$\\mathbf{W}_k^{\\prime}$ is the refined projection matrix $\\mathbf{e}^f_i$ is the embedding of a word co-existing with the target phrase $\\mathbf{e}_i^g$ is the embedding of that word when the target phrase is replaced with its super-category or a generic concept $\\mathbf{e}_i^p$ is the embedding for preserving prior knowledge $\\lambda_1$ is a hyperparameter The closed-form solution for this optimization problem is:\n$$ \\mathbf{W}k^\\prime = \\left( \\sum{i=1}^n \\mathbf{W}_k \\cdot \\mathbf{e}^g_i \\cdot (\\mathbf{e}i^f)^T + \\lambda_1 \\sum{i=n+1}^{n+m} \\mathbf{W}k \\cdot \\mathbf{e}^p_i \\cdot (\\mathbf{e}i^p)^T \\right) \\cdot \\left( \\sum{i=1}^n \\mathbf{e}^f_i \\cdot (\\mathbf{e}^f_i)^T + \\lambda_1 \\sum{i=n+1}^{n+m} \\mathbf{e}^p_i \\cdot (\\mathbf{e}_i^p)^T \\right)^{-1} $$\nThis refinement step helps to remove the residual information of the target concept from co-existing words, improving the overall efficacy of the erasure process.\n2. Target Concept Erasure with LoRA After refining the cross-attention layers, MACE focuses on erasing the intrinsic information within the target phrase itself. This is achieved using Low-Rank Adaptation (LoRA) modules [2], which provide an efficient way to fine-tune the model for specific concepts.\nThe key idea is to suppress the activation in certain regions of the attention maps that correspond to the target phrase tokens. The loss function for this step is:\n$$ \\min \\sum_{i \\in S} \\sum_{l}| \\mathbf{A}_{t,l}^i \\odot \\mathbf{M} |^2_F $$\nWhere:\n$S$ is the set of indices corresponding to the tokens of the target phrase $\\mathbf{A}_{t,l}^i$ is the attention map of token $i$ at layer $l$ and timestep $t$ $\\mathbf{M}$ is the segmentation mask obtained using Grounded-SAM [3, 4] $|\\cdot|_F$ is the Frobenius norm To implement this, MACE uses LoRA to decompose the weight modulations into low-rank matrices:\n$$ \\hat{\\mathbf{W}}_k = \\mathbf{W}_k^{\\prime} + \\Delta \\mathbf{W}_k = \\mathbf{W}_k^{\\prime} + \\mathbf{B} \\times \\mathbf{D} $$\nWhere $\\mathbf{B} \\in \\mathbb{R}^{d_\\text{in} \\times r}$ and $\\mathbf{D} \\in \\mathbb{R}^{r \\times d_\\text{out}}$, with $r \\ll \\min(d_\\text{in}, d_\\text{out})$ being the decomposition rank.\n3. Fusion of Multi-LoRA Modules The final step in MACE is to integrate multiple LoRA modules, each responsible for erasing a specific concept. Instead of using a simple weighted sum, which can lead to interference between modules, MACE introduces a novel fusion technique.\nThe objective function for this fusion is:\n$$ \\min_{\\mathbf{W}k^*} \\sum{i=1}^q \\sum_{j=1}^p | \\mathbf{W}k^* \\cdot \\mathbf{e}j^f - (\\mathbf{W}{k}^\\prime+ \\Delta \\mathbf{W}{k,i}) \\cdot \\mathbf{e}^f_j |2^2 + \\lambda_2 \\sum{j=p+1}^{p+m} | \\mathbf{W}_k^* \\cdot \\mathbf{e}_j^p - \\mathbf{W}_k \\cdot \\mathbf{e}^p_j |_2^2 $$\nWhere:\n$q$ is the number of erased concepts $p$ is the number of embeddings for mapping $m$ is the number of embeddings for preserving prior knowledge $\\lambda_2$ is a hyperparameter This fusion technique allows MACE to integrate multiple LoRA modules without mutual interference, preventing catastrophic forgetting and providing more flexibility in erasing multiple concepts.\nConcept-Focal Importance Sampling An important innovation in MACE is the introduction of Concept-Focal Importance Sampling (CFIS). This technique addresses the issue of maintaining specificity when erasing concepts that contain polysemous words or common names.\nInstead of sampling timesteps uniformly during LoRA training, MACE uses a probability density function that assigns greater probability to smaller values of $t$:\n$$ \\xi(t) = \\frac{1}{Z} \\left( \\sigma\\left( \\gamma(t-t_1) \\right) - \\sigma\\left( \\gamma(t-t_2) \\right) \\right) $$\nWhere:\n$Z$ is a normalizer $\\sigma(x)$ is the sigmoid function $t_1$ and $t_2$ are the bounds of a high probability sampling interval $\\gamma$ is a temperature hyperparameter This sampling strategy helps maintain specificity by focusing on the later stages of the diffusion process, where the specific mode of the concept is determined.\nExperimental Results The authors conducted extensive experiments to evaluate MACE across four different tasks:\nObject erasure Celebrity erasure Explicit content erasure Artistic style erasure Let’s look at some of the key results:\nObject Erasure MACE was tested on erasing the ten object classes from the CIFAR-10 dataset. The results show that MACE achieves the highest harmonic mean across nine out of ten object classes, demonstrating superior erasure capabilities while balancing specificity and generality.\nFigure 3: Qualitative comparison of airplane erasure\nCelebrity Erasure One of the most impressive aspects of MACE is its ability to erase multiple concepts simultaneously. In the celebrity erasure task, MACE was evaluated on erasing 1, 5, 10, and 100 celebrities.\nFigure 4: Qualitative comparison of erasing 100 celebrities\nThe results show that MACE significantly outperforms existing methods, especially when erasing 100 concepts. It maintains a good balance between efficacy and specificity, even for challenging cases like preserving “Bill Murray” while erasing “Bill Clinton”.\nExplicit Content Erasure MACE was also evaluated on its ability to mitigate the generation of explicit content. The results show that MACE generates the least amount of explicit content when conditioned on 4,703 prompts from the Inappropriate Image Prompt (I2P) dataset.\nArtistic Style Erasure In the task of erasing 100 artistic styles, MACE again demonstrates superior performance, achieving the highest overall erasing capability while maintaining good FID scores on regular content generation.\nImplementation Details For those interested in implementing MACE, here are some key details:\nThe authors used Stable Diffusion v1.4 as the base model Images were generated using the DDIM sampler over 50 steps Each LoRA module was trained for 50 gradient update steps The code is available at: https://github.com/Shilin-LU/MACE Conclusion and Future Directions MACE represents a significant advancement in the field of concept erasure for text-to-image diffusion models. Its ability to erase multiple concepts while maintaining a balance between generality and specificity opens up new possibilities for creating safer and more controlled AI-generated content.\nSome potential future directions for this work include:\nScaling up the erasure scope to handle thousands of concepts Exploring ways to further improve the specificity for highly related concepts Investigating the application of MACE to other types of generative models beyond text-to-image As AI-generated content becomes increasingly prevalent, techniques like MACE will play a crucial role in ensuring that these powerful tools can be used responsibly and ethically.\nReferences [1] Lu, S., Wang, Z., Li, L., Liu, Y., \u0026 Kong, A. W. (2024). MACE: Mass Concept Erasure in Diffusion Models. arXiv preprint.\n[2] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., … \u0026 Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.\n[3] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., … \u0026 Girshick, R. (2023). Segment Anything. arXiv preprint arXiv:2304.02643.\n[4] Liu, Y., Jia, X., Tan, J., Zhang, S., Xin, J., \u0026 Zhang, L. (2023). Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv preprint arXiv:2303.05499.\n[5] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., \u0026 Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684-10695).\n[6] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … \u0026 Polosukhin, I. (2017). Attention is All you Need. In Advances in Neural Information Processing Systems (pp. 5998-6008).\n","wordCount":"1533","inLanguage":"en","image":"https://example.org/%3Cimage%20path/url%3E","datePublished":"2020-09-15T11:30:03Z","dateModified":"2020-09-15T11:30:03Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://example.org/posts/mace/"},"publisher":{"@type":"Organization","name":"My New Hugo Site","logo":{"@type":"ImageObject","url":"https://example.org/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://example.org/ accesskey=h title="Home (Alt + H)"><img src=https://example.org/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://example.org/categories/ title=categories><span>categories</span></a></li><li><a href=https://example.org/tags/ title=tags><span>tags</span></a></li><li><a href=https://example.org title=example.org><span>example.org</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://example.org/>Home</a>&nbsp;»&nbsp;<a href=https://example.org/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">MACE: Mass Concept Erasure in Diffusion Models</h1><div class=post-description>Desc Text.</div><div class=post-meta><span title='2020-09-15 11:30:03 +0000 +0000'>September 15, 2020</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1533 words&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/mace/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#tldr>TLDR</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#the-challenge-of-concept-erasure>The Challenge of Concept Erasure</a></li><li><a href=#the-mace-framework>The MACE Framework</a><ul><li><a href=#1-closed-form-cross-attention-refinement>1. Closed-Form Cross-Attention Refinement</a></li><li><a href=#2-target-concept-erasure-with-lora>2. Target Concept Erasure with LoRA</a></li><li><a href=#3-fusion-of-multi-lora-modules>3. Fusion of Multi-LoRA Modules</a></li></ul></li><li><a href=#concept-focal-importance-sampling>Concept-Focal Importance Sampling</a></li><li><a href=#experimental-results>Experimental Results</a><ul><li><a href=#object-erasure>Object Erasure</a></li><li><a href=#celebrity-erasure>Celebrity Erasure</a></li><li><a href=#explicit-content-erasure>Explicit Content Erasure</a></li><li><a href=#artistic-style-erasure>Artistic Style Erasure</a></li></ul></li><li><a href=#implementation-details>Implementation Details</a></li><li><a href=#conclusion-and-future-directions>Conclusion and Future Directions</a></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><p><img loading=lazy src=0_overview.png#center alt="MACE Overview"></p><p><em>Figure 1: Overview of MACE framework</em></p><h2 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2><ul><li>MACE is a novel framework for erasing multiple concepts from text-to-image diffusion models</li><li>It achieves superior balance between generality and specificity compared to existing methods</li><li>MACE can effectively erase up to 100 concepts simultaneously without compromising model performance</li><li>The approach combines closed-form cross-attention refinement and concept-specific LoRA modules</li><li>MACE outperforms state-of-the-art methods across various concept erasure tasks</li></ul><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Text-to-image diffusion models have revolutionized the field of AI-generated art and content creation. However, their ability to generate almost any concept based on text prompts has raised concerns about potential misuse, copyright infringement, and the creation of harmful or explicit content. To address these issues, researchers have been exploring ways to selectively remove or &ldquo;erase&rdquo; specific concepts from these models.</p><p>In this blog post, we&rsquo;ll dive into a cutting-edge approach called MACE (MAss Concept Erasure) proposed by Shilin Lu et al. in their paper &ldquo;MACE: Mass Concept Erasure in Diffusion Models&rdquo; [1]. This work introduces a powerful framework for erasing multiple concepts from text-to-image diffusion models while maintaining a delicate balance between generality and specificity.</p><h2 id=the-challenge-of-concept-erasure>The Challenge of Concept Erasure<a hidden class=anchor aria-hidden=true href=#the-challenge-of-concept-erasure>#</a></h2><p>Before we delve into the details of MACE, let&rsquo;s understand the key challenges in concept erasure:</p><ol><li><strong>Efficacy</strong>: The model should not generate images related to the erased concept when prompted.</li><li><strong>Generality</strong>: The erasure should extend to synonyms and related terms of the target concept.</li><li><strong>Specificity</strong>: Unrelated concepts should remain intact and generate as expected.</li></ol><p>Previous methods have struggled to achieve all three of these goals simultaneously, especially when dealing with multiple concepts. MACE addresses these challenges through a novel combination of techniques.</p><h2 id=the-mace-framework>The MACE Framework<a hidden class=anchor aria-hidden=true href=#the-mace-framework>#</a></h2><p>MACE consists of three main components:</p><ol><li>Closed-Form Cross-Attention Refinement</li><li>Target Concept Erasure with LoRA</li><li>Fusion of Multi-LoRA Modules</li></ol><p>Let&rsquo;s explore each of these components in detail.</p><h3 id=1-closed-form-cross-attention-refinement>1. Closed-Form Cross-Attention Refinement<a hidden class=anchor aria-hidden=true href=#1-closed-form-cross-attention-refinement>#</a></h3><p>The first step in MACE is to refine the cross-attention layers of the pretrained model. This is crucial because the information of a concept is not only contained within its own tokens but also embedded in co-existing words through the attention mechanism.</p><p><img loading=lazy src=1_evoke.png alt="Residual Information"></p><p><em>Figure 2: A concept can be generated solely via residual information</em></p><p>To address this, MACE uses a closed-form solution to update the projection matrices $\mathbf{W}_k$ and $\mathbf{W}_v$ in the cross-attention layers. The objective is to discourage the model from embedding residual information of the target phrase into other words.</p><p>The optimization problem is formulated as:</p><p>$$
\min_{\mathbf{W}^{\prime}<em>k} \sum</em>{i=1}^n | \mathbf{W}_k^{\prime} \cdot \mathbf{e}^f_i - \mathbf{W}_k \cdot \mathbf{e}_i^g |<em>2^2 + \lambda_1 \sum</em>{i=n+1}^{n+m} | \mathbf{W}_k^{\prime} \cdot \mathbf{e}^p_i - \mathbf{W}_k \cdot \mathbf{e}_i^p |_2^2
$$</p><p>Where:</p><ul><li>$\mathbf{W}_k^{\prime}$ is the refined projection matrix</li><li>$\mathbf{e}^f_i$ is the embedding of a word co-existing with the target phrase</li><li>$\mathbf{e}_i^g$ is the embedding of that word when the target phrase is replaced with its super-category or a generic concept</li><li>$\mathbf{e}_i^p$ is the embedding for preserving prior knowledge</li><li>$\lambda_1$ is a hyperparameter</li></ul><p>The closed-form solution for this optimization problem is:</p><p>$$
\mathbf{W}<em>k^\prime = \left( \sum</em>{i=1}^n \mathbf{W}_k \cdot \mathbf{e}^g_i \cdot (\mathbf{e}<em>i^f)^T + \lambda_1 \sum</em>{i=n+1}^{n+m} \mathbf{W}<em>k \cdot \mathbf{e}^p_i \cdot (\mathbf{e}<em>i^p)^T \right) \cdot \left( \sum</em>{i=1}^n \mathbf{e}^f_i \cdot (\mathbf{e}^f_i)^T + \lambda_1 \sum</em>{i=n+1}^{n+m} \mathbf{e}^p_i \cdot (\mathbf{e}_i^p)^T \right)^{-1}
$$</p><p>This refinement step helps to remove the residual information of the target concept from co-existing words, improving the overall efficacy of the erasure process.</p><h3 id=2-target-concept-erasure-with-lora>2. Target Concept Erasure with LoRA<a hidden class=anchor aria-hidden=true href=#2-target-concept-erasure-with-lora>#</a></h3><p>After refining the cross-attention layers, MACE focuses on erasing the intrinsic information within the target phrase itself. This is achieved using Low-Rank Adaptation (LoRA) modules [2], which provide an efficient way to fine-tune the model for specific concepts.</p><p>The key idea is to suppress the activation in certain regions of the attention maps that correspond to the target phrase tokens. The loss function for this step is:</p><p>$$
\min \sum_{i \in S} \sum_{l}| \mathbf{A}_{t,l}^i \odot \mathbf{M} |^2_F
$$</p><p>Where:</p><ul><li>$S$ is the set of indices corresponding to the tokens of the target phrase</li><li>$\mathbf{A}_{t,l}^i$ is the attention map of token $i$ at layer $l$ and timestep $t$</li><li>$\mathbf{M}$ is the segmentation mask obtained using Grounded-SAM [3, 4]</li><li>$|\cdot|_F$ is the Frobenius norm</li></ul><p>To implement this, MACE uses LoRA to decompose the weight modulations into low-rank matrices:</p><p>$$
\hat{\mathbf{W}}_k = \mathbf{W}_k^{\prime} + \Delta \mathbf{W}_k = \mathbf{W}_k^{\prime} + \mathbf{B} \times \mathbf{D}
$$</p><p>Where $\mathbf{B} \in \mathbb{R}^{d_\text{in} \times r}$ and $\mathbf{D} \in \mathbb{R}^{r \times d_\text{out}}$, with $r \ll \min(d_\text{in}, d_\text{out})$ being the decomposition rank.</p><h3 id=3-fusion-of-multi-lora-modules>3. Fusion of Multi-LoRA Modules<a hidden class=anchor aria-hidden=true href=#3-fusion-of-multi-lora-modules>#</a></h3><p>The final step in MACE is to integrate multiple LoRA modules, each responsible for erasing a specific concept. Instead of using a simple weighted sum, which can lead to interference between modules, MACE introduces a novel fusion technique.</p><p>The objective function for this fusion is:</p><p>$$
\min_{\mathbf{W}<em>k^*} \sum</em>{i=1}^q \sum_{j=1}^p | \mathbf{W}<em>k^* \cdot \mathbf{e}<em>j^f - (\mathbf{W}</em>{k}^\prime+ \Delta \mathbf{W}</em>{k,i}) \cdot \mathbf{e}^f_j |<em>2^2 + \lambda_2 \sum</em>{j=p+1}^{p+m} | \mathbf{W}_k^* \cdot \mathbf{e}_j^p - \mathbf{W}_k \cdot \mathbf{e}^p_j |_2^2
$$</p><p>Where:</p><ul><li>$q$ is the number of erased concepts</li><li>$p$ is the number of embeddings for mapping</li><li>$m$ is the number of embeddings for preserving prior knowledge</li><li>$\lambda_2$ is a hyperparameter</li></ul><p>This fusion technique allows MACE to integrate multiple LoRA modules without mutual interference, preventing catastrophic forgetting and providing more flexibility in erasing multiple concepts.</p><h2 id=concept-focal-importance-sampling>Concept-Focal Importance Sampling<a hidden class=anchor aria-hidden=true href=#concept-focal-importance-sampling>#</a></h2><p>An important innovation in MACE is the introduction of Concept-Focal Importance Sampling (CFIS). This technique addresses the issue of maintaining specificity when erasing concepts that contain polysemous words or common names.</p><p>Instead of sampling timesteps uniformly during LoRA training, MACE uses a probability density function that assigns greater probability to smaller values of $t$:</p><p>$$
\xi(t) = \frac{1}{Z} \left( \sigma\left( \gamma(t-t_1) \right) - \sigma\left( \gamma(t-t_2) \right) \right)
$$</p><p>Where:</p><ul><li>$Z$ is a normalizer</li><li>$\sigma(x)$ is the sigmoid function</li><li>$t_1$ and $t_2$ are the bounds of a high probability sampling interval</li><li>$\gamma$ is a temperature hyperparameter</li></ul><p>This sampling strategy helps maintain specificity by focusing on the later stages of the diffusion process, where the specific mode of the concept is determined.</p><h2 id=experimental-results>Experimental Results<a hidden class=anchor aria-hidden=true href=#experimental-results>#</a></h2><p>The authors conducted extensive experiments to evaluate MACE across four different tasks:</p><ol><li>Object erasure</li><li>Celebrity erasure</li><li>Explicit content erasure</li><li>Artistic style erasure</li></ol><p>Let&rsquo;s look at some of the key results:</p><h3 id=object-erasure>Object Erasure<a hidden class=anchor aria-hidden=true href=#object-erasure>#</a></h3><p>MACE was tested on erasing the ten object classes from the CIFAR-10 dataset. The results show that MACE achieves the highest harmonic mean across nine out of ten object classes, demonstrating superior erasure capabilities while balancing specificity and generality.</p><p><img loading=lazy src=2_appendix_airplane.png alt="Object Erasure Results"></p><p><em>Figure 3: Qualitative comparison of airplane erasure</em></p><h3 id=celebrity-erasure>Celebrity Erasure<a hidden class=anchor aria-hidden=true href=#celebrity-erasure>#</a></h3><p>One of the most impressive aspects of MACE is its ability to erase multiple concepts simultaneously. In the celebrity erasure task, MACE was evaluated on erasing 1, 5, 10, and 100 celebrities.</p><p><img loading=lazy src=3_cele_erasure.png alt="Celebrity Erasure Results"></p><p><em>Figure 4: Qualitative comparison of erasing 100 celebrities</em></p><p>The results show that MACE significantly outperforms existing methods, especially when erasing 100 concepts. It maintains a good balance between efficacy and specificity, even for challenging cases like preserving &ldquo;Bill Murray&rdquo; while erasing &ldquo;Bill Clinton&rdquo;.</p><h3 id=explicit-content-erasure>Explicit Content Erasure<a hidden class=anchor aria-hidden=true href=#explicit-content-erasure>#</a></h3><p>MACE was also evaluated on its ability to mitigate the generation of explicit content. The results show that MACE generates the least amount of explicit content when conditioned on 4,703 prompts from the Inappropriate Image Prompt (I2P) dataset.</p><h3 id=artistic-style-erasure>Artistic Style Erasure<a hidden class=anchor aria-hidden=true href=#artistic-style-erasure>#</a></h3><p>In the task of erasing 100 artistic styles, MACE again demonstrates superior performance, achieving the highest overall erasing capability while maintaining good FID scores on regular content generation.</p><h2 id=implementation-details>Implementation Details<a hidden class=anchor aria-hidden=true href=#implementation-details>#</a></h2><p>For those interested in implementing MACE, here are some key details:</p><ul><li>The authors used Stable Diffusion v1.4 as the base model</li><li>Images were generated using the DDIM sampler over 50 steps</li><li>Each LoRA module was trained for 50 gradient update steps</li><li>The code is available at: <a href=https://github.com/Shilin-LU/MACE>https://github.com/Shilin-LU/MACE</a></li></ul><h2 id=conclusion-and-future-directions>Conclusion and Future Directions<a hidden class=anchor aria-hidden=true href=#conclusion-and-future-directions>#</a></h2><p>MACE represents a significant advancement in the field of concept erasure for text-to-image diffusion models. Its ability to erase multiple concepts while maintaining a balance between generality and specificity opens up new possibilities for creating safer and more controlled AI-generated content.</p><p>Some potential future directions for this work include:</p><ol><li>Scaling up the erasure scope to handle thousands of concepts</li><li>Exploring ways to further improve the specificity for highly related concepts</li><li>Investigating the application of MACE to other types of generative models beyond text-to-image</li></ol><p>As AI-generated content becomes increasingly prevalent, techniques like MACE will play a crucial role in ensuring that these powerful tools can be used responsibly and ethically.</p><hr><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Lu, S., Wang, Z., Li, L., Liu, Y., & Kong, A. W. (2024). MACE: Mass Concept Erasure in Diffusion Models. arXiv preprint.</p><p>[2] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &mldr; & Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</p><p>[3] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., &mldr; & Girshick, R. (2023). Segment Anything. arXiv preprint arXiv:2304.02643.</p><p>[4] Liu, Y., Jia, X., Tan, J., Zhang, S., Xin, J., & Zhang, L. (2023). Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection. arXiv preprint arXiv:2303.05499.</p><p>[5] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 10684-10695).</p><p>[6] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &mldr; & Polosukhin, I. (2017). Attention is All you Need. In Advances in Neural Information Processing Systems (pp. 5998-6008).</p><hr></div><footer class=post-footer><ul class=post-tags><li><a href=https://example.org/tags/mace/>Mace</a></li></ul><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on x" href="https://x.com/intent/tweet/?text=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models&amp;url=https%3a%2f%2fexample.org%2fposts%2fmace%2f&amp;hashtags=mace"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fexample.org%2fposts%2fmace%2f&amp;title=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models&amp;summary=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models&amp;source=https%3a%2f%2fexample.org%2fposts%2fmace%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fexample.org%2fposts%2fmace%2f&title=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fexample.org%2fposts%2fmace%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on whatsapp" href="https://api.whatsapp.com/send?text=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models%20-%20https%3a%2f%2fexample.org%2fposts%2fmace%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on telegram" href="https://telegram.me/share/url?text=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models&amp;url=https%3a%2f%2fexample.org%2fposts%2fmace%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share MACE: Mass Concept Erasure in Diffusion Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=MACE%3a%20Mass%20Concept%20Erasure%20in%20Diffusion%20Models&u=https%3a%2f%2fexample.org%2fposts%2fmace%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://example.org/>My New Hugo Site</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>